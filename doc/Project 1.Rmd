---
title: "Project 1"
author: "Qingyang Zhong (qz2317)"
date: "9/19/2018"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Part 1: Preprocess

Firstly, we need to process the raw textual data preparing for the analysis, which has been done by the starter code provided by Prof.Tian.

##Step 0: Load packages

```{r,warning=FALSE, message=FALSE}

packages.used=c("plyr","tm","tidytext","tidyverse","DT","wordcloud","scales","wordcloud2","gplots","ngram","dplyr","qdap","syuzhet","ggplot2","topicmodels","gridExtra","shiny","igraph","text2vec","data.table","magrittr","glmnet","reshape")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
library(shiny)
library(ggplot2)
library(plyr)
library(dplyr)
library(tm)
library(igraph)
library(text2vec)
library(data.table)
library(magrittr)
library(glmnet)
library(wordcloud2)
library(topicmodels)
library(gplots)
library(reshape)

```


##Step 1: Load processed data along with demographic information on contributors and combine them

```{r,warning=FALSE, message=FALSE}
hm_data <- read_csv("../output/processed_moments.csv")

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)

hm_data <- hm_data %>%
  inner_join(demo_data, by = "wid") %>%
  select(wid,
         original_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         text) %>%
  mutate(count = sapply(hm_data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))
```


#Part 2: Key words

In this section, firstly we will find out the key words about these moment.

##Step 1: Create a word cloud with top 200 key words.

```{r,warning=FALSE, message=FALSE}

wordcount <- hm_data$text %>% 
  str_trim() %>% 
  lapply(strsplit," ") %>% 
  unlist() %>% 
  table() %>% 
  as.data.frame()

colnames(wordcount) <- c("Keyword","Freq")

sorted <- wordcount %>% dplyr::arrange(desc(Freq))

png(filename = ".../output/wordcloud.png")

wordcloud2(sorted[1:200,],color = "random-light", size = 0.5,shape = "circle")

dev.off()

```

##Step 2: Show the frequency of the top 50 key words in bar chart.

```{r}
keywords50 <- head(sorted, n=50)

ggplot(keywords50,aes(fill=keywords50$Keyword)) +
  geom_bar(aes(x=keywords50$Keyword,y=keywords50$Freq),stat='identity',fill="pink")+
  xlab('Top 50 key words')+
  ylab('Frequency')+
  ggtitle('Frequency of top 50 key words')+
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
  scale_x_discrete(limits= keywords50$Keyword) + 
  guides(fill=FALSE)

```

We can find that from both graphs above, the frequent key words come up in these moment are related to friend and family.


#Part 3: Analysis Regarding Nationality

##Step 1: Omit the data points that are not having country information
```{r}
hm_data <- hm_data[!is.na(hm_data$country),] 
```

##Step 2: Find the distribution about nationality in this dataset

```{r}
count(hm_data$country)[count(hm_data$country)$freq>500,]
```


```{r}
country<-as.data.frame(table(hm_data$country))
sortcountry<-country %>% dplyr::arrange(desc(Freq))
sortcountry<-head(sortcountry,n=10)
ggplot(sortcountry,aes(fill=sortcountry$Var1)) +
  geom_bar(aes(x=sortcountry$Var1,y=sortcountry$Freq),stat='identity',fill="lightpink")+
  xlab('Top 10 countries')+
  ylab('Frequency')+
  ggtitle('Count of Top 10 Nationalities')+
  scale_x_discrete(limits= sortcountry$Var1) + 
  guides(fill=FALSE)
```

We can see that the marjority is from USA and IND, so we are going to do some comparison between these 2 countries. And see if nationality plays a part in happy moments.

##Step 3: Frequency of keywords in USA and India

```{r}

usa <- hm_data[which(hm_data$country=="USA"),]
usa_word <- usa$text %>% 
  str_trim() %>% 
  lapply(strsplit," ") %>% 
  unlist() %>%
  table() %>% 
  as.data.frame() %>% 
  dplyr::arrange(desc(Freq))

ind <- hm_data[which(hm_data$country=="IND"),]
ind_word <- ind$text %>% 
  str_trim() %>% 
  lapply(strsplit," ") %>% 
  unlist() %>% 
  table() %>% 
  as.data.frame() %>% 
  dplyr::arrange(desc(Freq))

top10usa<-  usa_word[1:10,]

names(top10usa) <- c("keywords","freq")

g_usa<-ggplot(top10usa) + geom_bar(aes(x=top10usa$keywords,y=top10usa$freq),
        position="dodge",stat="identity",
        width=0.6,fill="red")+
        scale_x_discrete(limits= rev(top10usa$keywords))+
        guides(fill=F)+
        ggtitle("USA People")+
        coord_flip()

top10ind<- ind_word[1:10,]

names(top10ind) <- c("keywords","freq")

g_ind<-ggplot(top10ind) + geom_bar(aes(x=top10ind$keywords,y=top10ind$freq),
        position="dodge",stat="identity",
        width=0.6,fill="steelblue")+
        scale_x_discrete(limits= rev(top10ind$keywords))+
        guides(fill=F)+
        ggtitle("IND People")+
        coord_flip()

grid.arrange(g_usa,g_ind,ncol=2,nrow=1)

```

From the chart above shows the frequent words used by USA and IND.

##Step 4: Comparation between key words of USA and India

A comparison cloud compares the relative frequency with which a term was used in two or more groups and it plots the difference between the word usage in the documents.

```{r}

hm_data_country<-hm_data[hm_data$country=='USA'|hm_data$country=='IND',]

bag_of_words <-  hm_data_country %>% unnest_tokens(word, text)

wordcount2 <- bag_of_words %>%group_by(country)%>%dplyr::count(word, sort = TRUE)

tdm2 <- cast(wordcount2,word~country,value = 'n')
tdm2[is.na(tdm2)] <- 0
rownames(tdm2) <- tdm2$word
tdm2$word <- NULL

comparison.cloud(tdm2, random.order=FALSE,colors=c('lightpink','lightblue'),
                 title.size=2, max.words=60)
```

From the comparison cloud, we can see that words like 'life' and 'day' were more front-and-center in India than in USA.
While 'watched', 'dog' is large in the USA side.

