---
title: "Project 1"
author: "Qingyang Zhong (qz2317)"
date: "9/19/2018"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Part 1: Preprocess

Firstly, we need to process the raw textual data preparing for the analysis, which has been done by the starter code provided by Prof.Tian.

##Step 0: Load packages

```{r,warning=FALSE, message=FALSE}

packages.used=c("plyr","tm","tidytext","tidyverse","DT","wordcloud","scales","wordcloud2","gplots","ngram","dplyr","qdap","syuzhet","ggplot2","topicmodels","gridExtra","shiny","igraph","text2vec","data.table","magrittr","glmnet","reshape")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
library(shiny)
library(ggplot2)
library(plyr)
library(dplyr)
library(tm)
library(igraph)
library(text2vec)
library(data.table)
library(magrittr)
library(glmnet)
library(wordcloud2)
library(topicmodels)
library(gplots)
library(reshape)

```


##Step 1: Load processed data along with demographic information on contributors and combine them

```{r,warning=FALSE, message=FALSE}
hm_data <- read_csv("../output/processed_moments.csv")

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)

hm_data <- hm_data %>%
  inner_join(demo_data, by = "wid") %>%
  select(wid,
         original_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         text) %>%
  mutate(count = sapply(hm_data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))
```


#Part 2: Topic Modeling

In this section, firstly we will find out the key words about these moment. Then use LDA to do some topic modeling to find out the topic that comes frequently in these moment.

##Step 1: Create a word cloud with top 200 key words.

```{r,warning=FALSE, message=FALSE}

wordcount <- hm_data$text %>% 
  str_trim() %>% 
  lapply(strsplit," ") %>% 
  unlist() %>% 
  table() %>% 
  as.data.frame()

colnames(wordcount) <- c("Keyword","Freq")

sorted <- wordcount %>% dplyr::arrange(desc(Freq))

png(filename = ".../output/wordcloud.png")

wordcloud2(sorted[1:200,],color = "random-light", size = 0.5,shape = "circle")

dev.off()

```

##Step 2: Show the frequency of the top 50 key words in bar chart.

```{r}
keywords50 <- head(sorted, n=50)

ggplot(keywords50,aes(fill=keywords50$Keyword)) +
  geom_bar(aes(x=keywords50$Keyword,y=keywords50$Freq),stat='identity',fill="pink")+
  xlab('Top 50 key words')+
  ylab('Frequency')+
  ggtitle('Frequency of top 50 key words')+
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
  scale_x_discrete(limits= keywords50$Keyword) + 
  guides(fill=FALSE)

```

We can find that from both graphs above, the frequent key words come up in these moment are related to friend and family.

##Step 3: LDA
```{r}
dtm <- VCorpus(VectorSource(hm_data$text)) %>%DocumentTermMatrix()
rowTotals <- slam::row_sums(dtm)
dtm <- dtm[rowTotals > 0, ]

#Number of topics
topic <- 10

#Run LDA 
ldaOut <- LDA(dtm,topic,method="Gibbs",control=list(nstart=5,seed = list(2003,5,63,100001,765),best=TRUE,burnin = 800, iter = 400, thin=100))

#write out results
ldaOut.topics <- as.matrix(topics(ldaOut))
# Total number per each topic
table(c(1:k, ldaOut.topics)) 

#top 10 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,10))
ldaOut.terms  

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:10]])
}


```


##Step 4: Based on the result from LDA, conclude top 10 topic from words. Then mark each topic.

```{r}

top10topic=c("Work","Family","Vacation","Pets","People","Celebration","Social","Entertainment","School","Exercise")

hm_data <- hm_data[rowTotals > 0, ]
hm_data$ldatopic <- as.vector(ldaOut.topics)
hm_data$ldahash <-top10topic[ldaOut.topics]
colnames(topicProbabilities) <- top10topic

hm_data.df <- cbind(hm_data, topicProbabilities)


wordcount_by_topic <- hm_data.df %>%
  unnest_tokens(word, text) %>%
  group_by(lda) %>%
  dplyr::count(word, sort = TRUE)

wordcount_by_topic %>% 
  slice(1:10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n, color = lda)) + geom_col() +  facet_wrap(~lda, scales = "free") + xlab(NULL) + ylab("Word Frequency")+ coord_flip()

```


#Part 3: Analysis Regarding Nationality

##Step 1: Omit the data points that are not having country information
```{r}
hm_data <- hm_data[!is.na(hm_data$country),] 
```

##Step 2: Find the distribution about nationality in this dataset

```{r}
count(hm_data$country)[count(hm_data$country)$freq>500,]
```


```{r}
country<-as.data.frame(table(hm_data$country))
sortcountry<-country %>% dplyr::arrange(desc(Freq))
sortcountry<-head(sortcountry,n=10)
ggplot(sortcountry,aes(fill=sortcountry$Var1)) +
  geom_bar(aes(x=sortcountry$Var1,y=sortcountry$Freq),stat='identity',fill="lightpink")+
  xlab('Top 10 countries')+
  ylab('Frequency')+
  ggtitle('Count of Top 10 Nationalities')+
  scale_x_discrete(limits= sortcountry$Var1) + 
  guides(fill=FALSE)
```

We can see that the marjority is from USA and IND, so we are going to do some comparison between these 2 countries. And see if nationality plays a part in happy moments.

##Step 3: Frequency of keywords in USA and India

```{r}

usa <- hm_data[which(hm_data$country=="USA"),]
usa_word <- usa$text %>% 
  str_trim() %>% 
  lapply(strsplit," ") %>% 
  unlist() %>%
  table() %>% 
  as.data.frame() %>% 
  dplyr::arrange(desc(Freq))

ind <- hm_data[which(hm_data$country=="IND"),]
ind_word <- ind$text %>% 
  str_trim() %>% 
  lapply(strsplit," ") %>% 
  unlist() %>% 
  table() %>% 
  as.data.frame() %>% 
  dplyr::arrange(desc(Freq))

top10usa<-  usa_word[1:10,]

names(top10usa) <- c("keywords","freq")

g_usa<-ggplot(top10usa) + geom_bar(aes(x=top10usa$keywords,y=top10usa$freq),
        position="dodge",stat="identity",
        width=0.6,fill="red")+
        scale_x_discrete(limits= rev(top10usa$keywords))+
        guides(fill=F)+
        ggtitle("USA People")+
        coord_flip()

top10ind<- ind_word[1:10,]

names(top10ind) <- c("keywords","freq")

g_ind<-ggplot(top10ind) + geom_bar(aes(x=top10ind$keywords,y=top10ind$freq),
        position="dodge",stat="identity",
        width=0.6,fill="steelblue")+
        scale_x_discrete(limits= rev(top10ind$keywords))+
        guides(fill=F)+
        ggtitle("IND People")+
        coord_flip()

grid.arrange(g_usa,g_ind,ncol=2,nrow=1)

```

From the chart above shows the frequent words used by USA and IND.

##Step 4: Comparation between key words of USA and India

A comparison cloud compares the relative frequency with which a term was used in two or more groups and it plots the difference between the word usage in the documents.

```{r}

hm_data_country<-hm_data[hm_data$country=='USA'|hm_data$country=='IND',]

bag_of_words <-  hm_data_country %>% unnest_tokens(word, text)

wordcount2 <- bag_of_words %>%group_by(country)%>%dplyr::count(word, sort = TRUE)

tdm2 <- cast(wordcount2,word~country,value = 'n')
tdm2[is.na(tdm2)] <- 0
rownames(tdm2) <- tdm2$word
tdm2$word <- NULL

comparison.cloud(tdm2, random.order=FALSE,colors=c('lightpink','lightblue'),
                 title.size=2, max.words=60)
```

From the comparison cloud, we can see that words like 'life' and 'day' were more front-and-center in India than in USA.


##Step 5: Heat map

```{r}

usatopic<-as.data.frame(table(usa$ldatopic))
indtopic<-as.data.frame(table(ind$ldatopic))
colnames(indtopic)<-c('Topic','ind')
colnames(usatopic)<-c('Topic','usa')

countrysummary<-merge(indtopic,usatopic,by='Topic')

heatmap.2(as.matrix(countrysummary[,-1]), Rowv = FALSE,
           scale = "column", key=F, na.rm = T,
           labRow = c("Work","Family","Vacation","Pets","People","Celebration","Social","Entertainment","School","Exercise"),
          labCol = c('IND','USA'),
          dendrogram="none",
           srtCol=0, adjCol=c(1,1)
          )

```
According to the heat map we can see that the topic difference between Indian and USA's happy moment.
